name: Performance Regression Detection

on:
  workflow_dispatch: {}
  pull_request:
    branches:
      - main
      - "**"
    paths:
      - 'FoundationUI/**/*.swift'
      - 'FoundationUI/Package.swift'
      - 'Examples/ComponentTestApp/**/*.swift'
      - '.github/workflows/performance-regression.yml'
  push:
    branches:
      - main
    paths:
      - 'FoundationUI/**/*.swift'
      - 'FoundationUI/Package.swift'
      - 'Examples/ComponentTestApp/**/*.swift'

# Permissions for commenting on PRs
permissions:
  contents: read
  pull-requests: write

# Cancel in-progress runs for same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-metrics:
    name: Measure Build Performance
    runs-on: macos-15
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Select Xcode 26.0
        uses: maxim-lobanov/setup-xcode@v1
        with:
          xcode-version: "26.0"

      - name: Show versions
        run: |
          xcodebuild -version
          swift --version

      - name: Measure SPM build time (FoundationUI)
        id: build-time
        run: |
          set -eo pipefail
          cd FoundationUI

          echo "üìä Measuring Swift Package build time..."

          # Clean build
          swift package clean

          # Time the build using Python (works on macOS and Linux)
          python3 << 'PYTHON_TIMING'
          import subprocess
          import time
          import sys
          import os

          start = time.time()
          result = subprocess.run(["swift", "build"], capture_output=False)
          end = time.time()

          if result.returncode != 0:
              sys.exit(result.returncode)

          duration_s = end - start
          duration_ms = int(duration_s * 1000)

          print(f"Build time: {duration_s:.2f}s ({duration_ms}ms)")

          with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
              f.write(f"build_time_ms={duration_ms}\n")
              f.write(f"build_time_s={duration_s:.2f}\n")
          PYTHON_TIMING

          # Also output via echo for visibility
          python3 << 'PYTHON_OUTPUT'
          import os
          import time
          print(f"Build completed at {time.strftime('%Y-%m-%d %H:%M:%S')}")
          PYTHON_OUTPUT

      - name: Measure library and build artifact size
        id: binary-size
        run: |
          set -eo pipefail
          cd FoundationUI

          # Build in release mode
          swift build -c release

          # For library packages, measure the build directory size and SwiftModule
          BUILD_DIR="./.build/release"

          if [ -d "$BUILD_DIR" ]; then
            # Get total build directory size
            TOTAL_SIZE=$(find "$BUILD_DIR" -type f -exec stat -c%s {} \; 2>/dev/null | awk '{sum+=$1} END {print sum}')

            # Find and measure the .swiftmodule
            SWIFTMODULE_SIZE=0
            if [ -f "$BUILD_DIR/FoundationUI.swiftmodule" ]; then
              SWIFTMODULE_SIZE=$(stat -c%s "$BUILD_DIR/FoundationUI.swiftmodule" 2>/dev/null || echo "0")
            fi

            # Find library files (.a on macOS/Linux, .dylib on macOS)
            LIB_SIZE=0
            if [ -f "$BUILD_DIR/libFoundationUI.a" ]; then
              LIB_SIZE=$(stat -c%s "$BUILD_DIR/libFoundationUI.a" 2>/dev/null || echo "0")
            elif [ -f "$BUILD_DIR/libFoundationUI.dylib" ]; then
              LIB_SIZE=$(stat -c%s "$BUILD_DIR/libFoundationUI.dylib" 2>/dev/null || echo "0")
            fi

            if [ -z "$TOTAL_SIZE" ] || [ "$TOTAL_SIZE" = "0" ]; then
              TOTAL_SIZE=0
            fi

            SIZE_KB=$(echo "scale=2; $TOTAL_SIZE / 1024" | bc)
            SIZE_MB=$(echo "scale=2; $TOTAL_SIZE / 1024 / 1024" | bc)

            echo "üì¶ Build artifacts:"
            echo "  Total size: ${SIZE_MB}MB (${SIZE_KB}KB)"
            echo "  Library: ${LIB_SIZE} bytes"
            echo "  SwiftModule: ${SWIFTMODULE_SIZE} bytes"

            echo "binary_size_bytes=$TOTAL_SIZE" >> $GITHUB_OUTPUT
            echo "binary_size_mb=$SIZE_MB" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è Build directory not found"
            echo "binary_size_bytes=0" >> $GITHUB_OUTPUT
            echo "binary_size_mb=0" >> $GITHUB_OUTPUT
          fi

      - name: Run unit tests with performance baseline
        id: test-performance
        continue-on-error: true
        run: |
          set -eo pipefail
          cd FoundationUI

          echo "‚è±Ô∏è Running unit tests with performance monitoring..."

          # Run tests with timing using Python (works on macOS and Linux)
          python3 << 'PYTHON_TEST_TIMING'
          import subprocess
          import time
          import sys
          import os

          start = time.time()
          result = subprocess.run(["swift", "test"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
          end = time.time()

          # Write output log
          with open('/tmp/test-output.log', 'w') as f:
              f.write(result.stdout)

          # Print to console
          print(result.stdout)

          # Calculate duration
          duration_s = end - start
          duration_ms = int(duration_s * 1000)

          print(f"Test execution time: {duration_s:.2f}s")

          # Write to GITHUB_OUTPUT
          with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
              f.write(f"test_duration_s={duration_s:.2f}\n")

          # Check test results
          if "passed" in result.stdout.lower() and result.returncode == 0:
              print("‚úÖ All tests completed")
              with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
                  f.write("tests_passed=true\n")
          else:
              print("‚ö†Ô∏è Tests may have issues, checking logs...")
              with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
                  f.write("tests_passed=false\n")
          PYTHON_TEST_TIMING

      - name: Store performance metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics
          path: |
            /tmp/test-output.log
          if-no-files-found: ignore
          retention-days: 30

      - name: Comment on PR with Performance Results
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const buildTime = "${{ steps.build-time.outputs.build_time_s }}";
            const binarySize = "${{ steps.binary-size.outputs.binary_size_mb }}";
            const testDuration = "${{ steps.test-performance.outputs.test_duration_s }}";
            const testsPassed = "${{ steps.test-performance.outputs.tests_passed }}";

            let comment = "## üìä Performance Metrics Report\n\n";

            if (buildTime && buildTime !== "0") {
              comment += `### Build Performance\n`;
              comment += `- **Build Time**: ${buildTime}s\n`;
              comment += `- **Target**: <120s (acceptable)\n`;
              comment += `- **Status**: ${parseFloat(buildTime) < 120 ? "‚úÖ Pass" : "‚ö†Ô∏è Monitor"}\n\n`;
            }

            if (binarySize && binarySize !== "0") {
              comment += `### Build Artifacts Size\n`;
              comment += `- **Total Size**: ${binarySize}MB\n`;
              comment += `- **Target**: <15MB (acceptable)\n`;
              comment += `- **Status**: ${parseFloat(binarySize) < 15 ? "‚úÖ Pass" : "‚ö†Ô∏è Monitor"}\n\n`;
            }

            if (testDuration && testDuration !== "0") {
              comment += `### Test Execution\n`;
              comment += `- **Duration**: ${testDuration}s\n`;
              comment += `- **Target**: <30s (acceptable)\n`;
              const status = testsPassed === "true" ? "‚úÖ Passed" : "‚ö†Ô∏è Check logs";
              comment += `- **Result**: ${status}\n\n`;
            }

            comment += `### Quality Gates\n`;
            comment += `- **Build Time**: <120s\n`;
            comment += `- **Artifact Size**: <15MB\n`;
            comment += `- **Test Execution**: <30s\n`;
            comment += `- **Unit Tests**: All passing\n\n`;

            comment += `### Next Steps\n`;
            comment += `- Monitor performance over multiple builds\n`;
            comment += `- For detailed profiling: See \`FoundationUI/DOCS/INPROGRESS/blocked.md\`\n`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  test-runtime-performance:
    name: Unit Test Execution Performance
    runs-on: macos-15
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Select Xcode 26.0
        uses: maxim-lobanov/setup-xcode@v1
        with:
          xcode-version: "26.0"

      - name: Run FoundationUI tests with timing
        id: test-time
        continue-on-error: true
        run: |
          set -eo pipefail
          cd FoundationUI

          echo "‚è±Ô∏è Measuring unit test execution time..."

          # Run tests and capture timing
          time swift test

          echo "‚úÖ Test execution completed"

      - name: Upload test logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-performance-logs
          path: |
            /tmp/test-*.log
          if-no-files-found: ignore
          retention-days: 30

  regression-detection:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [build-metrics, test-runtime-performance]
    if: always()
    steps:
      - name: Check for regressions
        run: |
          echo "## üìà Performance Regression Detection"
          echo ""
          echo "### Build Metrics"
          echo "- ‚úÖ Build time monitored"
          echo "- ‚úÖ Binary size tracked"
          echo "- ‚è≥ Baseline comparison (stored in artifacts)"
          echo ""
          echo "### Unit Test Performance"
          echo "- ‚úÖ Test execution time measured"
          echo "- ‚úÖ Individual test timings available in logs"
          echo ""
          echo "### Detailed Analysis"
          echo "For detailed performance profiling and bottleneck analysis:"
          echo "1. Download performance artifacts from this workflow"
          echo "2. Follow instructions in Phase 5.2 blocked.md"
          echo "3. Use Xcode Instruments for detailed profiling"
          echo "   - Time Profiler: CPU usage"
          echo "   - Allocations: Memory usage"
          echo "   - Core Animation: Frame rate"
